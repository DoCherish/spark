job_name,job_url,job_location,job_salary,job_company,job_experience,job_class,job_given,job_detail,company_type,company_person,search_key,city
大数据初级开发工程师,https://www.lagou.com/jobs/7047496.html,河东区,8k-11k,上海诺悦智能科技有限公司,1-3年,大专,五险一金 奖金 餐补 通讯补助等,任职要求：1. 精通Scala、java语言，有Scala实际编程经验；2. 精通传统数据库（ORACLE、MySQL等）的存储过程的编写；3. 掌握SparkCore/SparkSQL的设计和框架；4. 掌握Hadoop、HIVE的数据的解析、传输和存储；5. 掌握ORC列式文件的相关使用；6. 深入了解分布式系统、大数据平台、消息队列等高可用高弹性架构;7. 对HDFS/Yarn/HBase/Hive/Spark相关组件的性能优化和补丁跟踪等有实际项目经验者优先；8. 熟悉银行业务，有过银行数据处理、分析经验者优先；9. 在Hadoop、Spark、Storm、Flink等开源社区有过贡献最佳；10. 具有良好的英语读写；11. 本科及以上学历，2年以上相关工作经验；,数据服务,50-150人,spark,天津
大数据开发工程师,https://www.lagou.com/jobs/6426863.html,北辰区,15k-20k,天津市拓甫网络科技开发有限公司,5-10年,本科,五险一金、周末双休、年底双薪、年度体检,"岗位职责：1、满足业务需求，基于海量数据实现数据采集、清洗入库、统计计算、Web展示。2、主要负责实时计算分析和离线数据统计分析；3、构建推荐模型、排序优化系统的设计和开发；4、构建数据管理平台,用于存储、计算、分析、挖掘用户行为等日志数据处理和分析；5、利用挖掘技术分析用户偏好特征，构建用户画像、精细化运营等业务的数据基础。任职要求：1、本科及以上学历，有5年以上大数据开发经验；2、熟练掌握Java以及Scala/Python(2者任意一种)语言，熟练Linux开发环境以及Shell、Python常用脚本语言；3、熟练使用flink,Hadoop，Spark，Storm，Hive，Hbase，elasticsearch，MongoDB等开源技术；4、在实时计算、流计算、多维度olap分析方面有实践经验；5、有3年以上团队管理能力，具有团队合作精神，能独立解决遇到技术难题。","移动互联网,信息安全",50-150人,spark,天津
大数据开发工程师,https://www.lagou.com/jobs/6029864.html,河西区,15k-20k,中交智运有限公司,3-5年,本科,央企福利,"岗位职责：1、参与公司大数据系统研发工作；2、参与发掘和分析业务需求，进行系统方案设计；3、参与系统代码编写，确保性能、质量和安全；任职要求：1、全日制本科及以上学历，中共党员优先考虑，计算机相关专业，3年以上IT从业经验；2、精通Java 开发语言，熟悉常见的web前端开发；3、优先考虑有大数据平台Hadoop/Storm/Spark开发经验，熟悉Spark, Hive或Pig原理与实现；4、具有良好的抽象设计能力，思路清晰，善于思考，能独立分析和解决问题,责任心强，具备良好的团队合作精神和承受压力的能力；5、学习能力强，适应能力好，对技术、互联网有热情。","移动互联网,物流丨运输",2000人以上,spark,天津
大数据工程师,https://www.lagou.com/jobs/7098335.html,西青区,8k-15k,天地伟业技术有限公司,3-5年,本科,"奖金丰厚,福利待遇好,各种团建,发展空间","岗位职责:1、为涉及大数据技术的产品提供大数据服务，设计实现大数据应用业务    2、对现有大数据技术进行改进持续提升大数据服务支撑水平 3、对大数据新技术调研并将新技术应用到实际的大数据产品中4、设计并实现大数据基础架构，实现架构的性能和通用性、稳定性，扩展性的持续升级任职资格:1、精通Java 开发语言。    2、熟悉常见的web前端开发。3、构架支持超大数量级的数据的大数据平台系统，及相关监控，拓展系统设计及实现。4、精通Hadoop及其生态圈相关组件，比如MapReduce、HDFS、Hive、Impala、Spark等；5、优先考虑有大数据平台Hadoop/Hive/Storm/Spark/Flink/Kafka 开发经验，熟悉Spark分布式计算, cassandra/hbase/solr/Elasticsearch等数据库； 6、具有良好的抽象设计能力，思路清晰，善于思考，能独立分析和解决问题,责任心强，具备良好的团队合作精神和承受压力的能力；7、有大型分布式系统开发经验优先、有大型互联网工作经验优先；8、对现有系统的进行架构深入分析及系统优化，进一步提升系统的性能及数据处理能力；9、学习能力强，适应能力好，对技术、互联网、云计算、云安全有热情；","硬件,移动互联网",500-2000人,spark,天津
IT大数据平台基础架构工程师,https://www.lagou.com/jobs/6817715.html,和平区,15k-30k,捷信消费金融有限公司,5-10年,本科,五险一金 带薪休假 补充商业保险,"Duties and responsibilities:The Sr. Data Platform Infrastructure Engineer will ensure the stability of Homecredit data service platform which is based on big data technology (HDFS, HBase, Kudu, Impala, Hive, Spark, etc.), by optimizing platform performance efficiency and setting up monitoring system that will detect abnormal activities damaging platform performance. The candidate will leverage both their technology and leadership skillsets as they partner across the Infrastructure Delivery Organization to architect, design, implement and automate an infrastructure-as-a-service model for our data service platform.Requirements:Deep experiences managing the Big Data DBMS within a large enterprise environment;Hands-on architectural experience designing, implementing;And operationalizing Big Data DBMS based on Cloudera, Hortonwork, and Apache technologies;Good knowledge in cluster computing, strong experiences performing and optimizing common Big Data platform administration tasks (at both software and hardware level) in support of service delivery;Strong track record of successful use of the following Big Data DBMS functionalities: HDFS, HBase, Kudu, Impala, Hive, Spark, etc.;Hands-On experiences understand and analyze common Big Data technical logs;Hands-on experiences with DevOps database automation practices;Understand how Hive script, Impala script, Spark script impact cluster performance;Understand the mechanism of HDFS, HBase, Kudu, etc. and resource management among different components.Staff Welfare:After on boarding, providing supplementary commercial insurance besides the state-stipulated insurance and housing fund;Fast and transparent promotion channel and broad development opportunities;5-15 days paid annual leave and 5 days paid sick leave. Free annual physical check-up;Attractive employee activities.",金融,2000人以上,spark,天津
大数据开发工程师,https://www.lagou.com/jobs/7055006.html,和平区,14k-27k,捷信消费金融有限公司,3-5年,本科,五险一金 带薪休假 补充商业保险,"Duties and responsibilities： l  Deploy business use cases that runs one JD/Taobao like Decision Engine and Search Engine；l  Technical savvy as much as business savvy；l  Being able to guide business who are not familiar with the technical design of DE/SE and design proper business use cases that will easy to implement, and logical to end EShop users；Being able to create KPIs that will be used to monitor system performance.Requirements:l  Deep knowledge of Scala on Spark，Maven/SBT;l  Deep knowledge of Python on scikit-learn, Pandas;l  Deep knowledge of data warehouse data modeling，Spark-SQL/Hive-SQL/PL-SQL;l  Deep knowledge of Vim, Linux;l  Deep knowledge of Decision Tree, LR, GBDT, DBSCAN, Fast-Unfolding, K-means, Data cube, etc and their respective optimization;l  Deep knowledge of XGBoost, LightGBM, Tensorflow framework;Being able to develop with Spark ML. Staff Welfare:1. After on boarding, providing supplementary commercial insurance besides the state-stipulated insurance and housing fund;2. Fast and transparent promotion channel and broad development opportunities;3. 5-15 days paid annual leave and 5 days paid sick leave. Free annual physical check-up;4. Attractive employee activities.",金融,2000人以上,spark,天津
大数据开发工程师,https://www.lagou.com/jobs/7087027.html,南开区,11k-20k,天津开发区先特网络系统有限公司,5-10年,本科,天津信息安全**，竞争力的薪资，大牛带队,岗位职责：1、负责大数据平台的开发和优化，基于大数据平台完成项目开发任务；2、负责海量数据的采集、清洗、挖掘，优化大数据计算性能，实时数据计算的开发。岗位要求：1、有实际的产品或项目开发经验熟练java/python/Go等任意一种语言;2、熟练使用Hive、flume、kafka、mongodb、redis、spark、ElasticSearch、hbase组件进行开发，熟悉分布式系统的工作机制者;3、熟悉攻防模型和安全数据（例如网络流量数据，系统安全收集的数据等）的分析和使用；有在大规模分布式数据处理的架构下使用分析数据的经验；4、学习能力强，责任心强，抗压力强，具有较强的思考和解决问题的能力，具有较强的团队沟通和协作能力。,"硬件,数据服务",150-500人,spark,天津
大数据开发工程师,https://www.lagou.com/jobs/6747484.html,滨海新区,15k-30k,渣打环球商业服务有限公司,5-10年,不限,六险一金 带薪年假 节日福利,"The Role Responsibilities：- Development of large scale big data solutions to be used in a very large production environment.- Gather, analyze and maintain large data sets to provide answers to address hurdles and create innovative solutions in large-scale data infrastructures.- Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology.- Develop code and mentor junior developers to ensure deliverables are on time, within budget, and with good code quality.- Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.Our Ideal Candidate- Demonstrated hands on experience building Big Data solutions. Experience with Hadoop ecosystem: Hadoop, Spark, MapReduce, Hive/Pig, Yarn, HBase, etc- Experience with RDBMS as well as NoSQL stacks (ElasticSearch, HBase, Cassandra)- Good grasp of statistical and scientific programming packages in Python is a plus- Experience with data visualization tools: Hue, Kibana, Qlikview, Tableau is a plus- Good grasp of data science concepts with emphasis on machine learning techniques is a plus- Independent problem-solving, highly motivated and self-directing- Experience with Agile/Scrum development methodologies- Software testing know-how, including black-box and white-box testing methodologies and both functional and non-functional testing- Test driven development experience- Understanding of DevOps practices- Aptitude to learn the business domain of the application systems to be developed- Strong communication skills, should be able to communicate effectively with business and other stake holders",金融,500-2000人,spark,天津
